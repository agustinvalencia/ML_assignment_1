---
title: "Machine Learning Assignment 1"
author: "Agust√≠n Valencia - aguva779"
date: "11/19/2019"
output: pdf_document
---

```{r setup, include=FALSE, warning=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(openxlsx)
library(kknn)
library(ggplot2)
library(MASS)
library(glmnet)
```

# Assignment 1. Spam classification with nearest neighbors

```{r, echo=FALSE}
data <- read.xlsx("data/spambase.xlsx")
n = dim(data)[1]
set.seed(12345)
id = sample(1:n, floor(n*0.5))
train = data[id,]
test = data[-id,]
```

### 2. Use logistic regression to classify the training and test data by the classification principle $\hat{Y} = 1$ if $p(Y=1 | X ) > 0.5$, otherwise $\hat{Y} = 0$ and report the confusion matrices and the misclassification rates for train and test data. Analyze the obtained results. 


```{r, warning=FALSE, echo=FALSE}
# util function
get_performance <- function(targets, predictions, text) {
    cat("Classification Performance :", text, "\n")
    t <- table(targets, predictions)
    #print("Confusion Matrix")
    #print(t)
    tn <- t[1,1]
    tp <- t[2,2]
    fp <- t[1,2]
    fn <- t[2,1]
    total <- dim(test)[1]
    tpr <- tp/(tp+fp) * 100
    tnr <- tn/(tn+fn) * 100
    fpr <- fp/(tp+fp) * 100
    fnr <- fn/(tn+fn) * 100
    
    #cat("Rates details:\n")
    cat(" TPR =", tpr, "% -")
    cat(" TNR =", tnr, "% -")
    cat(" FPR =", fpr, "% -")
    cat(" FNR =", fnr, "%")
    cat("\n Misclassification Rate = ", (fp+fn)/total * 100, "%\n")
}

# fit the model
fit <- glm(Spam ~ . , data = train, family = "binomial")
```

Evaluating the model with training data :

```{r, warning=FALSE, echo=FALSE}
# performance on training data
pred_train <- predict(fit, newdata = train)
pred_train_at_05 <- as.integer(pred_train > 0.5)
targets <- train$Spam
get_performance(targets, pred_train_at_05, "train set - trigger = 0.5")
```

Now, with unseen data it can be observed that the misclassification rate increased, though numbers still consistent.

```{r, warning=FALSE, echo=FALSE}
# performance on test data
pred_test <- predict(fit, newdata = test)
pred_test_at_05  <- as.integer(pred_test > 0.5)
targets <- test$Spam
get_performance(targets, pred_test_at_05, "test set - trigger = 0.5")
```


### 3. Use logistic regression to classify the test data by the classification principle  $\hat{Y} = 1$ if $p(Y=1 | X ) > 0.8$, otherwise $\hat{Y} = 0$ 

Setting a higher trigger implies that the classifier will be more selective, then it is expected to decrease the amount of mails being labeled as spam. 

The training stats:

```{r, echo=FALSE}
# performance on train data
pred_train_at_08  <- as.integer(pred_train > 0.8)
targets <- train$Spam
get_performance(targets, pred_train_at_08, "train set - trigger = 0.8")
```

Testing stats: 

```{r, echo=FALSE}
# performance on test data
pred_test_at_08  <- as.integer(pred_test > 0.8)
targets <- test$Spam
get_performance(targets, pred_test_at_08, "test set - trigger = 0.8")
```

Although the misclassification rate has increased, the false positive rate, i.e., the amount of valid email being sent to the spambox, has decreased, which from a user perspective could be more valuable than a higher accuracy on true positives. 

### 4. Use standard kknn() with K = 30 from package *kknn*, report the misclassification rates for the training and test data and compare the results with step 2.

```{r, warning=FALSE, echo=FALSE}
# Train KNN K=30
knn_model <- train.kknn(Spam ~ . , data = train, ks = 30)

# performance on training data
knn_fit <- predict(knn_model, train)
results <- as.integer(knn_fit > 0.5)
target <- train$Spam
get_performance(target, results, "train knn - k = 30")

# performance on test data
knn_fit <- predict(knn_model, test)
results <- as.integer(knn_fit > 0.5)
target <- test$Spam
get_performance(target, results, "test knn - k = 30")
```


### 5. Repeat step 4 for K=1 and compare results with step 4. What effects does the decrease of K lead to and why?

```{r, warning=FALSE, echo=FALSE}
# Train KNN K=1
knn_model <- train.kknn(Spam ~ . , data = train, ks = 1)

# performance on training data
knn_fit <- predict(knn_model, train)
results <- as.integer(knn_fit > 0.5)
target <- train$Spam
get_performance(target, results, "train knn - k = 1")

# performance on test data
knn_fit <- predict(knn_model, test)
results <- as.integer(knn_fit > 0.5)
target <- test$Spam
get_performance(target, results, "test knn - k = 1")
```

If we assign k=1 training misclassification is 0%, this means we are overfitting our model, thus the misclassification for the testing set may be bigger than other scenarios. 





# Assignment 3. Feature selection by cross-validation in a linear model. 

### 1. Implement an R function that performs feature selection (best subset selection) in linear regression by using k-fold cross-validation without using any specialized function like lm() (**use only basic R functions**) your function should depend on: 
- $X$: Matrix containing X measurements.
- $Y$: Vector containing Y measurements.
- $Nfolds$: number of folds in the cross-validation.

You may assume in your code that matrix $X$ has 5 columns. The function should plot the CV scores computed for various feature subsets against the number of features, and it should  also return the optimal subset of features and the corresponding cross-validation (CV) score. Before splitting into folds, the data should be permuted and the seed 12345 should be used for that purpose. 

```{r}

```

















# Assignment 4. Linear regression and regularization

### 1. Import data and create a plot of Moisture versus Protein. Do you think these data are described well by a linear model? 

```{r, echo=FALSE, out.height='30%', fig.align='center'}
## Import data and plot Moisture vs Protein.
data <- read.xlsx("data/tecator.xlsx")
plot(data$Protein, data$Moisture)
```

By the plot, although there are some outliers, it seems that the data could be approximated by a linear model. 

### 2. Consider model $M_i$ in which Moisture is normally distributed and the expected Moisture is polynomial 

$$M_{i} = \sum_{j=0}^{i}{\beta_j x^j} + \varepsilon$$
$$i = {1,\cdots, 6} $$
$$\varepsilon \sim N(\mu,\sigma^2)$$

### 3. Divide the data (50/50) and fit models $M_i, i=1,\cdots, 6$. For each model, record the training and validation MSE and present a plot showing how training and validation MSE depend on $i$. Which model is best according to this plot? How do MSE values change and why? Interpret this picture in bias-variance tradeoff.

```{r, echo=FALSE, out.height='30%', fig.align='center'}
# Creating data sets
p <- data$Protein
y <- data$Moisture
P <- data.frame(
    Y = y,
    P1 = p, 
    P2 = p^2, 
    P3 = p^3, 
    P4 = p^4, 
    P5 = p^5, 
    P6 = p^6 
)

n = dim(P)[1]
set.seed(12345)
id = sample(1:n, floor(n*0.5))
train = P[id,]
test = P[-id,]

# Training models
M1 <- lm(Y ~ ., data = train[,1:2])
M2 <- lm(Y ~ ., data = train[,1:3])
M3 <- lm(Y ~ ., data = train[,1:4])
M4 <- lm(Y ~ ., data = train[,1:5])
M5 <- lm(Y ~ ., data = train[,1:6])
M6 <- lm(Y ~ ., data = train[,1:7])

## Train Scores
eval_model <- function(model, data) {
  pred <- predict(model, data)
  errors <- pred - data$Y
  MSE <- mean(errors^2)
  return(MSE)
}

MSE_train <- c()
MSE_train[1] <- eval_model(M1, train)
MSE_train[2] <- eval_model(M2, train)
MSE_train[3] <- eval_model(M3, train)
MSE_train[4] <- eval_model(M4, train)
MSE_train[5] <- eval_model(M5, train)
MSE_train[6] <- eval_model(M6, train)

MSE_test <- c()
MSE_test[1] <- eval_model(M1, test)
MSE_test[2] <- eval_model(M2, test)
MSE_test[3] <- eval_model(M3, test)
MSE_test[4] <- eval_model(M4, test)
MSE_test[5] <- eval_model(M5, test)
MSE_test[6] <- eval_model(M6, test)

df <- data.frame(
    degree <- c(1:6),
    MSE_train,
    MSE_test
)
p <- ggplot()
p <- p +geom_point(data = df, aes( x = degree, y = MSE_train, color="Train")) + 
    geom_line(data = df, aes( x = degree, y = MSE_train, color="Train"))
p <- p + geom_point(data = df, aes( x = degree, y = MSE_test, color="Test")) + 
    geom_line(data = df, aes( x = degree, y = MSE_test, color="Test"))
p <- p + labs(y="MSE", colour="Dataset", title = "Mean Square Errors") + geom_line() 
p

```

### 4. Perform variable selection of a linear model in which Fat is response and Channel1-Channel100 are predictors by using stepAIC. Comment on how many variables were selected.

```{r, echo=FALSE, results="hide"}
adhok_data <- data[,2:101]
Fat <- data$Fat
adhok_data <- cbind(adhok_data, Fat)
model <- lm(Fat ~ . , data=adhok_data) 
step <- stepAIC(model, direction ="both")
selected_vars <- step$coefficients
```

After running stepAIC we get that the amount of selected variables is :

```{r, echo=FALSE}
cat("There were selected", length(selected_vars), "variables\n")
```

Thus, taking into account that one of them is the intercept, we have 63 selected variables out of 100. 


### 5. Fit a Ridge regression model with the same predictor and response variables. Present a plot showing how model coefficients depend on the log of the penalty factor $\lambda$ and report how the coefficients change with $\lambda$

```{r, echo=FALSE, out.height='30%', fig.align='center'}
covariates <- scale(adhok_data[1:(length(adhok_data)-1)])
response <- scale(adhok_data$Fat)
model_ridge <- glmnet(covariates, response, alpha = 0, family = "gaussian")
plot(model_ridge, xvar="lambda", label=T)
```

It can be seen that when using Ridge regression among the $\lambda$ increasing, coefficients converge to zero, though the amount of parameters still 100 since Ridge do not drop them. 


### 6. Repeat step 5 but fit with LASSO instead of the Ridge regression and compare the plots from steps 5 and 6. Conclusions? 

```{r, echo=FALSE, out.height='30%', fig.align='center'}
model_lasso <- glmnet(covariates, response, alpha = 1, family = "gaussian")
plot(model_lasso, xvar="lambda", label=T)
```

LASSO converges to zero much faster than Ridge regression and it also drops variables. 


### 7. Use cross-validation to find optimal LASSO model (make sure that case $\lambda=0$ is also considered by the procedure), report the optimal $\lambda$ and how many variables were chosen by the model and make conclusions. Present also a plot showing the dependence of the CV score and comment how the CV score changes $\lambda$

```{r, echo=FALSE, out.height='30%', fig.align='center'}
lambdas <- append(model_lasso$lambda,0)
cv_model_lasso <- cv.glmnet(as.matrix(covariates), response, alpha=1, family="gaussian", lambda=lambdas)
cat("The best performance was at lambda = ", cv_model_lasso$lambda.min, "\n")
cv_model_lasso
plot(cv_model_lasso)
```


From the cross-validated model it is seen that the  $\lambda$ value for which it is obtained the minimum MSE score is at $\lambda_{min} = 0$. The model has 100 non-zero parameters. Along $\log(\lambda)$ increases MSE also increases


### 8. Compare the results from steps 4 and 7.

For (4) after performing a stepAIC over the model 36 variables were dropped, getting a final model with only 63 variables (plus the intercept). Nonetheless, for (7) after cross-validating the LASSO model it has been found that the best performance regarding MSE scores is at $\lambda=0$ which implies no penalization, thus, no parameters will be dropped.
























 